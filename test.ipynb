{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_dataset import PlantVillageDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "from utils.preprocessing import preprocessing_img\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB3, InceptionResNetV2\n",
    "import torchvision.models  as models\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "PATH = './Plant_leave_diseases_dataset_without_augmentation'\n",
    "transform = transforms.Compose([\n",
    "    preprocessing_img,\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "training_data = PlantVillageDataset(PATH, img_mode=\"LAB\", train=True, transform=transform)\n",
    "test_data = PlantVillageDataset(PATH, img_mode=\"LAB\", train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 224, 224) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;28mlist\u001b[39m(training_data\u001b[38;5;241m.\u001b[39mlabel_to_idx\u001b[38;5;241m.\u001b[39mkeys())[label])\n\u001b[0;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\matplotlib\\pyplot.py:3476\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   3455\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[0;32m   3456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[0;32m   3457\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3474\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3475\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[1;32m-> 3476\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m gca()\u001b[38;5;241m.\u001b[39mimshow(\n\u001b[0;32m   3477\u001b[0m         X,\n\u001b[0;32m   3478\u001b[0m         cmap\u001b[38;5;241m=\u001b[39mcmap,\n\u001b[0;32m   3479\u001b[0m         norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m   3480\u001b[0m         aspect\u001b[38;5;241m=\u001b[39maspect,\n\u001b[0;32m   3481\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[0;32m   3482\u001b[0m         alpha\u001b[38;5;241m=\u001b[39malpha,\n\u001b[0;32m   3483\u001b[0m         vmin\u001b[38;5;241m=\u001b[39mvmin,\n\u001b[0;32m   3484\u001b[0m         vmax\u001b[38;5;241m=\u001b[39mvmax,\n\u001b[0;32m   3485\u001b[0m         origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[0;32m   3486\u001b[0m         extent\u001b[38;5;241m=\u001b[39mextent,\n\u001b[0;32m   3487\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   3488\u001b[0m         filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[0;32m   3489\u001b[0m         filterrad\u001b[38;5;241m=\u001b[39mfilterrad,\n\u001b[0;32m   3490\u001b[0m         resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   3491\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   3492\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3493\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3494\u001b[0m     )\n\u001b[0;32m   3495\u001b[0m     sci(__ret)\n\u001b[0;32m   3496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\matplotlib\\__init__.py:1501\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1501\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\n\u001b[0;32m   1502\u001b[0m             ax,\n\u001b[0;32m   1503\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args),\n\u001b[0;32m   1504\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: sanitize_sequence(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m   1506\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1507\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1508\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m-> 5895\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\matplotlib\\image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\matplotlib\\image.py:697\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m    695\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    703\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (3, 224, 224) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(10, 10))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(list(training_data.label_to_idx.keys())[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50v2 = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "efficientnet_b0 = models.efficientnet_b0(weights='EfficientNet_B0_Weights.DEFAULT')\n",
    "efficientnet_b3 = models.efficientnet_b3(weights='EfficientNet_B3_Weights.DEFAULT')\n",
    "densenet = models.densenet121(weights='DenseNet121_Weights.DEFAULT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50v2 = resnet50v2.to(device)\n",
    "efficientnet_b0 = efficientnet_b0.to(device)\n",
    "efficientnet_b3 = efficientnet_b3.to(device)\n",
    "densenet = densenet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Model\n",
    "import torch.nn as nn\n",
    "class LinearHeadModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.25):\n",
    "        super(LinearHeadModel, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.batch_norm = nn.BatchNorm1d(input_dim)\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(pre_trained_model, linear_model, train_loader, criterion, optimizer, epochs=50):\n",
    "    linear_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs_pre_trained = pre_trained_model(inputs)\n",
    "            outputs = linear_model(outputs_pre_trained)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[3171][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x0000019BBFC03E50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input dimension should be at least 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m resnet_linear_block \u001b[38;5;241m=\u001b[39m LinearHeadModel(\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m38\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet50v2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet_linear_block\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet50v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(pre_trained_model, linear_model, train_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      8\u001b[0m outputs_pre_trained \u001b[38;5;241m=\u001b[39m pre_trained_model(inputs)\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_pre_trained\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mLinearHeadModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm(x)\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:1455\u001b[0m, in \u001b[0;36mAdaptiveAvgPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_avg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\torch\\nn\\functional.py:1381\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[1;34m(input, output_size)\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m   1380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, output_size)\n\u001b[1;32m-> 1381\u001b[0m _output_size \u001b[38;5;241m=\u001b[39m \u001b[43m_list_with_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(\u001b[38;5;28minput\u001b[39m, _output_size)\n",
      "File \u001b[1;32mc:\\Users\\tlc29\\Documents\\CS\\3A\\Deep Learning\\DL\\lib\\site-packages\\torch\\nn\\modules\\utils.py:41\u001b[0m, in \u001b[0;36m_list_with_default\u001b[1;34m(out_size, defaults)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out_size\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(defaults) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out_size):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput dimension should be at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(out_size)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     43\u001b[0m     v \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m d \u001b[38;5;28;01mfor\u001b[39;00m v, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(out_size, defaults[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(out_size) :])\n\u001b[0;32m     44\u001b[0m ]\n",
      "\u001b[1;31mValueError\u001b[0m: Input dimension should be at least 3"
     ]
    }
   ],
   "source": [
    "resnet_linear_block = LinearHeadModel(1000, 38)\n",
    "train_model(resnet50v2, resnet_linear_block, train_dataloader, nn.CrossEntropyLoss(), torch.optim.Adam(resnet50v2.parameters(), lr=0.001))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
